{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Assignment 4: Named entity recognition\nCreate a model for Named Entity Recognition for dataset CoNLL 2002.\nYour quality metric = f1_macro\n\nIn your solution you should use: RandomForest, Gradient Boosting (xgboost, lightgbm, catboost)\nTutorials:\n* https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide\n* https://github.com/catboost/tutorials\n\nMore baselines you beat - better your score\n\n* baseline 1 [3 points]: 0.0604 random labels\n* baseline 2 [5 points]: 0.3966 PoS features + logistic regression\n* baseline 3 [8 points]: 0.8122 word2vec cbow embedding + baseline 2 + svm\n\n[1 point] using feature engineering (creating features not presented in the baselines)\n\n! Your results must be reproducible. You should explicitly set all seeds random_states in yout model.\n! Remember to use proper training pipeline.\n\nbonus, think about:\n\n[1 point] Why did we select f1 score with macro averaging as our classification quality measure? What other metrics are suitable?"},{"metadata":{},"cell_type":"markdown","source":"# Ответ на дополнительный вопрос\n\nЗадачу распознавания именованных сущностей можно представить как задачу многоклассовой классификации. Один из классов определяется отрицательно, сюда относятся токены, которые не являются именованными сущностями, то есть их нельзя приписать никакому другому классу. Очевидно, что в текстах на любом естественном языке доля таких токенов обычно значительно больше доли токенов, которые входят в состав какой-либо именованной сущности.\n\nОтсюда следует, например, что метрика accuracy не подходит для задач NER: она недостаточно наказывает за false negative предсказания. Иными словами, модель, которая любой токен помечает как не-сущность, покажет довольно неплохие результаты по метрике accuracy. К тому же, неполное распознавание именованной сущности (e. g., только имя, хотя нужно «имя + фамилия») недостаточно сильно штрафуется. По схожим причинам обычно не используется AUC-ROC.\n\nПри оценке работы моделей, распознающих именованные сущности, довольно часто используют F1-метрику. Основное достоинство F1-метрики состоит в том, что она не зависит от объёма класса true negatives (не-сущности), который в задачах NER типично очень большой. Это позволяет учитывать способность выделять именованные сущности на фоне большого количества токенов, которые ими не являются, не страдая проблемой недостаточного предсказания.\n\nПомимо F1-метрики, можно также использовать по отдельности precision и recall, на расчёт которых опирается F1-метрика. Конечно, можно воспользоваться и AUC-PR. На данный момент разрабатываются и другие методы оценки качества работы NER-алгоритмов, например, token & separator model (Esuli & Sebastiani, 2010)."},{"metadata":{},"cell_type":"markdown","source":"# Решение задания\n\nВ работе использован алгоритм Word2Vec CBoW, RandomForestClassifier. Feature engineering не проводился."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn import model_selection\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import metrics\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\nSEED=1337","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/ner_short.csv\")\ndf.head()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"   Unnamed: 0 next-next-pos next-next-word next-pos      next-word  pos  \\\n0           0           NNS  demonstrators       IN             of  NNS   \n1           1           VBP           have      NNS  demonstrators   IN   \n2           2           VBN        marched      VBP           have  NNS   \n3           3            IN        through      VBN        marched  VBP   \n4           4           NNP         London       IN        through  VBN   \n\n     prev-pos prev-prev-pos prev-prev-word      prev-word  sentence_idx  \\\n0  __START1__    __START2__     __START2__     __START1__           1.0   \n1         NNS    __START1__     __START1__      Thousands           1.0   \n2          IN           NNS      Thousands             of           1.0   \n3         NNS            IN             of  demonstrators           1.0   \n4         VBP           NNS  demonstrators           have           1.0   \n\n            word tag  \n0      Thousands   O  \n1             of   O  \n2  demonstrators   O  \n3           have   O  \n4        marched   O  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>next-next-pos</th>\n      <th>next-next-word</th>\n      <th>next-pos</th>\n      <th>next-word</th>\n      <th>pos</th>\n      <th>prev-pos</th>\n      <th>prev-prev-pos</th>\n      <th>prev-prev-word</th>\n      <th>prev-word</th>\n      <th>sentence_idx</th>\n      <th>word</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NNS</td>\n      <td>demonstrators</td>\n      <td>IN</td>\n      <td>of</td>\n      <td>NNS</td>\n      <td>__START1__</td>\n      <td>__START2__</td>\n      <td>__START2__</td>\n      <td>__START1__</td>\n      <td>1.0</td>\n      <td>Thousands</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>VBP</td>\n      <td>have</td>\n      <td>NNS</td>\n      <td>demonstrators</td>\n      <td>IN</td>\n      <td>NNS</td>\n      <td>__START1__</td>\n      <td>__START1__</td>\n      <td>Thousands</td>\n      <td>1.0</td>\n      <td>of</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>VBN</td>\n      <td>marched</td>\n      <td>VBP</td>\n      <td>have</td>\n      <td>NNS</td>\n      <td>IN</td>\n      <td>NNS</td>\n      <td>Thousands</td>\n      <td>of</td>\n      <td>1.0</td>\n      <td>demonstrators</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>IN</td>\n      <td>through</td>\n      <td>VBN</td>\n      <td>marched</td>\n      <td>VBP</td>\n      <td>NNS</td>\n      <td>IN</td>\n      <td>of</td>\n      <td>demonstrators</td>\n      <td>1.0</td>\n      <td>have</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>NNP</td>\n      <td>London</td>\n      <td>IN</td>\n      <td>through</td>\n      <td>VBN</td>\n      <td>VBP</td>\n      <td>NNS</td>\n      <td>demonstrators</td>\n      <td>have</td>\n      <td>1.0</td>\n      <td>marched</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of sentences\ndf.sentence_idx.max()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"1500.0"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class distribution\ndf.tag.value_counts(normalize=True )","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"O        0.852828\nB-geo    0.027604\nB-gpe    0.020935\nB-org    0.020247\nI-per    0.017795\nB-tim    0.016927\nB-per    0.015312\nI-org    0.013937\nI-geo    0.005383\nI-tim    0.004247\nB-art    0.001376\nI-gpe    0.000837\nI-art    0.000748\nB-eve    0.000628\nI-eve    0.000508\nB-nat    0.000449\nI-nat    0.000239\nName: tag, dtype: float64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sentence length\ntdf = df.set_index('sentence_idx')\ntdf['length'] = df.groupby('sentence_idx').tag.count()\ndf = tdf.reset_index(drop=False)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encode categorial variables\n\nle = LabelEncoder()\ndf['pos'] = le.fit_transform(df.pos)\ndf['next-pos'] = le.fit_transform(df['next-pos'])\ndf['next-next-pos'] = le.fit_transform(df['next-next-pos'])\ndf['prev-pos'] = le.fit_transform(df['prev-pos'])\ndf['prev-prev-pos'] = le.fit_transform(df['prev-prev-pos'])","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"   sentence_idx  Unnamed: 0  next-next-pos next-next-word  next-pos  \\\n0           1.0           0             18  demonstrators         9   \n1           1.0           1             33           have        18   \n2           1.0           2             32        marched        33   \n3           1.0           3              9        through        32   \n4           1.0           4             16         London         9   \n\n       next-word  pos  prev-pos  prev-prev-pos prev-prev-word      prev-word  \\\n0             of   18        39             40     __START2__     __START1__   \n1  demonstrators    9        18             39     __START1__      Thousands   \n2           have   18         9             18      Thousands             of   \n3        marched   33        18              9             of  demonstrators   \n4        through   32        33             18  demonstrators           have   \n\n            word tag  length  \n0      Thousands   O      48  \n1             of   O      48  \n2  demonstrators   O      48  \n3           have   O      48  \n4        marched   O      48  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_idx</th>\n      <th>Unnamed: 0</th>\n      <th>next-next-pos</th>\n      <th>next-next-word</th>\n      <th>next-pos</th>\n      <th>next-word</th>\n      <th>pos</th>\n      <th>prev-pos</th>\n      <th>prev-prev-pos</th>\n      <th>prev-prev-word</th>\n      <th>prev-word</th>\n      <th>word</th>\n      <th>tag</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>0</td>\n      <td>18</td>\n      <td>demonstrators</td>\n      <td>9</td>\n      <td>of</td>\n      <td>18</td>\n      <td>39</td>\n      <td>40</td>\n      <td>__START2__</td>\n      <td>__START1__</td>\n      <td>Thousands</td>\n      <td>O</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>1</td>\n      <td>33</td>\n      <td>have</td>\n      <td>18</td>\n      <td>demonstrators</td>\n      <td>9</td>\n      <td>18</td>\n      <td>39</td>\n      <td>__START1__</td>\n      <td>Thousands</td>\n      <td>of</td>\n      <td>O</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>2</td>\n      <td>32</td>\n      <td>marched</td>\n      <td>33</td>\n      <td>have</td>\n      <td>18</td>\n      <td>9</td>\n      <td>18</td>\n      <td>Thousands</td>\n      <td>of</td>\n      <td>demonstrators</td>\n      <td>O</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>3</td>\n      <td>9</td>\n      <td>through</td>\n      <td>32</td>\n      <td>marched</td>\n      <td>33</td>\n      <td>18</td>\n      <td>9</td>\n      <td>of</td>\n      <td>demonstrators</td>\n      <td>have</td>\n      <td>O</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>4</td>\n      <td>16</td>\n      <td>London</td>\n      <td>9</td>\n      <td>through</td>\n      <td>32</td>\n      <td>33</td>\n      <td>18</td>\n      <td>demonstrators</td>\n      <td>have</td>\n      <td>marched</td>\n      <td>O</td>\n      <td>48</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting\ny = LabelEncoder().fit_transform(df.tag)\n\ndf_train, df_test, y_train, y_test = model_selection.train_test_split(df, y, stratify=y, \n                                                                      test_size=0.25, random_state=SEED, shuffle=True)\nprint('train', df_train.shape[0])\nprint('test', df_test.shape[0])","execution_count":8,"outputs":[{"output_type":"stream","text":"train 50155\ntest 16719\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some wrappers to work with word2vec\nfrom gensim.models.word2vec import Word2Vec\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.base import TransformerMixin\nfrom collections import defaultdict\n\n   \nclass Word2VecWrapper(TransformerMixin):\n    def __init__(self, window=5,negative=5, size=100, iter=100, is_cbow=False, random_state=SEED):\n        self.window_ = window\n        self.negative_ = negative\n        self.size_ = size\n        self.iter_ = iter\n        self.is_cbow_ = is_cbow\n        self.w2v = None\n        self.random_state = random_state\n        \n    def get_size(self):\n        return self.size_\n\n    def fit(self, X, y=None):\n        \"\"\"\n        X: list of strings\n        \"\"\"\n        sentences_list = [x.split() for x in X]\n        self.w2v = Word2Vec(sentences_list, \n                            window=self.window_,\n                            negative=self.negative_, \n                            size=self.size_, \n                            iter=self.iter_,\n                            sg=not self.is_cbow_, seed=self.random_state)\n\n        return self\n    \n    def has(self, word):\n        return word in self.w2v\n\n    def transform(self, X):\n        \"\"\"\n        X: a list of words\n        \"\"\"\n        if self.w2v is None:\n            raise Exception('model not fitted')\n        return np.array([self.w2v[w] if w in self.w2v else np.zeros(self.size_) for w in X ])\n    \n","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# here we exploit that word2vec is an unsupervised learning algorithm\n# so we can train it on the whole dataset (subject to discussion)\n\nsentences_list = [x.strip() for x in ' '.join(df.word).split('.')]\n\nw2v_cbow = Word2VecWrapper(window=5, negative=5, size=300, iter=300, is_cbow=True, random_state=SEED)\nw2v_cbow.fit(sentences_list)","execution_count":10,"outputs":[{"output_type":"stream","text":"CPU times: user 52.1 s, sys: 553 ms, total: 52.7 s\nWall time: 24.1 s\n","name":"stdout"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"<__main__.Word2VecWrapper at 0x7fe5a721db00>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# baseline 1 \n# random labels\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.dummy import DummyClassifier\n\n\ncolumns = ['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']\n\nmodel = Pipeline([\n    ('enc', OneHotEncoder()),\n    ('est', DummyClassifier(random_state=SEED)),\n])\n\nmodel.fit(df_train[columns], y_train)\n\nprint('train', metrics.f1_score(y_train, model.predict(df_train[columns]), average='macro'))\nprint('test', metrics.f1_score(y_test, model.predict(df_test[columns]), average='macro'))\n","execution_count":11,"outputs":[{"output_type":"stream","text":"train 0.05887736725599869\ntest 0.060439542712750365\nCPU times: user 208 ms, sys: 36.9 ms, total: 245 ms\nWall time: 245 ms\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# baseline 2 [GOT]\n# pos features + one hot encoding + random forest\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import ensemble\n\n\ncolumns = ['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']\n\nmodel = Pipeline([\n    ('enc', OneHotEncoder()),\n    ('est', ensemble.RandomForestClassifier(random_state=SEED)),\n])\n\nmodel.fit(df_train[columns], y_train)\n\nprint('train', metrics.f1_score(y_train, model.predict(df_train[columns]), average='macro'))\nprint('test', metrics.f1_score(y_test, model.predict(df_test[columns]), average='macro'))","execution_count":12,"outputs":[{"output_type":"stream","text":"train 0.7470525701346687\ntest 0.5994190204572042\nCPU times: user 25.2 s, sys: 121 ms, total: 25.3 s\nWall time: 25.4 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# baseline 2 [GOT]\n# pos features + one hot encoding + xgboost\nfrom sklearn.preprocessing import OneHotEncoder\nfrom xgboost import XGBClassifier\n\n\ncolumns = ['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']\n\nmodel = Pipeline([\n    ('enc', OneHotEncoder()),\n    ('est', XGBClassifier(random_state=SEED)),\n])\n\nmodel.fit(df_train[columns], y_train)\n\nprint('train', metrics.f1_score(y_train, model.predict(df_train[columns]), average='macro'))\nprint('test', metrics.f1_score(y_test, model.predict(df_test[columns]), average='macro'))","execution_count":13,"outputs":[{"output_type":"stream","text":"train 0.6467439915465034\ntest 0.42088356317601794\nCPU times: user 1min 8s, sys: 206 ms, total: 1min 8s\nWall time: 18.5 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# baseline 3 [GOT]\n# use word2vec cbow embedding + baseline 2 + RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nimport scipy.sparse as sp\nfrom sklearn import ensemble\n\nembedding = w2v_cbow\nencoder_pos = OneHotEncoder()\nX_train = sp.hstack([\n    embedding.transform(df_train.word),\n    embedding.transform(df_train['next-word']),\n    embedding.transform(df_train['next-next-word']),\n    embedding.transform(df_train['prev-word']),\n    embedding.transform(df_train['prev-prev-word']),\n    encoder_pos.fit_transform(df_train[['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n])\nX_test = sp.hstack([\n    embedding.transform(df_test.word),\n    embedding.transform(df_test['next-word']),\n    embedding.transform(df_test['next-next-word']),\n    embedding.transform(df_test['prev-word']),\n    embedding.transform(df_test['prev-prev-word']),\n    encoder_pos.transform(df_test[['pos','next-pos','next-next-pos','prev-pos','prev-prev-pos']])\n])\n\nmodel = ensemble.RandomForestClassifier(random_state=SEED)\nmodel.fit(X_train, y_train)\n\nprint('train', metrics.f1_score(y_train, model.predict(X_train), average='macro'))\nprint('test', metrics.f1_score(y_test, model.predict(X_test), average='macro'))","execution_count":14,"outputs":[{"output_type":"stream","text":"train 0.9942251490538192\ntest 0.8659334953852547\nCPU times: user 13min 16s, sys: 4.4 s, total: 13min 20s\nWall time: 13min 20s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":4}